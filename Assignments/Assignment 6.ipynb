{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f339c2",
   "metadata": {},
   "source": [
    "# Assignmnet 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1f765",
   "metadata": {},
   "source": [
    "# Text Summarisation for different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2d94a",
   "metadata": {},
   "source": [
    "## Japanese Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2af8d",
   "metadata": {},
   "source": [
    "## Import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4dafe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the stopwords corpus from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Importing the cosine_distance function from NLTK cluster utility\n",
    "from nltk.cluster.util import cosine_distance\n",
    "\n",
    "# Importing numpy library and aliasing it as 'np'\n",
    "import numpy as np\n",
    "\n",
    "# Importing the networkx library and aliasing it as 'nx'\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b11d67",
   "metadata": {},
   "source": [
    "## Open file and split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16f4e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿この言語の先史や、いつ日本に初めて登場したかについてはほとんど知られていません。西暦\n",
      "3\n",
      "世紀の中国の文書にはいくつかの日本語の単語が記録されていましたが、実質的な古日本語の文書は\n",
      "8\n",
      "世紀まで出現しませんでした。平安時代\n",
      "(794\n",
      "～\n",
      "1185\n",
      "年)\n",
      "から、中国と日本の語彙が大量に流入し、中期初期日本語の音韻論に影響を与えました。中期後期日本語\n",
      "(1185\n",
      "～\n",
      "1600\n",
      "年)\n",
      "では、文法が大幅に変更され、ヨーロッパからの外来語が初めて登場しました。標準語の基礎は近世日本の時代（17世紀前半～19世紀半ば）に関西地方から江戸地方（現在の東京）に移りました。\n",
      "1853\n",
      "年に日本の自主的鎖国が終了した後、ヨーロッパ言語からの外来語の流れが大幅に増加し、英語のルーツに由来する単語が急増しました。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Opening the text file in read mode\n",
    "file = open(\"C:\\\\Users\\\\himav\\\\Downloads\\\\Japanese.txt\", encoding=\"utf-8\")\n",
    "\n",
    "#This file contains one paragraph of multiple sentences\n",
    "filedata = file.readlines()\n",
    "\n",
    "# Extracting the first paragraph from the file data and splitting it into sentences\n",
    "article = filedata[0].split(\" \")\n",
    "\n",
    "# Initializing an empty list to store the tokenized sentences\n",
    "sentences = []\n",
    "\n",
    "# Looping through each sentence in the article\n",
    "for sentence in article:\n",
    "    # Printing the sentence\n",
    "    print(sentence)\n",
    "    # Tokenizing the sentence by replacing non-alphabetic characters with spaces and then splitting on spaces\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c1d0c",
   "metadata": {},
   "source": [
    "## Our data: a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3004733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['\\ufeffこの言語の先史や、いつ日本に初めて登場したかについてはほとんど知られていません。西暦'], ['3'], ['世紀の中国の文書にはいくつかの日本語の単語が記録されていましたが、実質的な古日本語の文書は'], ['8'], ['世紀まで出現しませんでした。平安時代'], ['(794'], ['～'], ['1185'], ['年)'], ['から、中国と日本の語彙が大量に流入し、中期初期日本語の音韻論に影響を与えました。中期後期日本語'], ['(1185'], ['～'], ['1600'], ['年)'], ['では、文法が大幅に変更され、ヨーロッパからの外来語が初めて登場しました。標準語の基礎は近世日本の時代（17世紀前半～19世紀半ば）に関西地方から江戸地方（現在の東京）に移りました。'], ['1853'], ['年に日本の自主的鎖国が終了した後、ヨーロッパ言語からの外来語の流れが大幅に増加し、英語のルーツに由来する単語が急増しました。\\n']]\n"
     ]
    }
   ],
   "source": [
    "# Printing the list of tokenized sentences\n",
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8939f5b6",
   "metadata": {},
   "source": [
    "## Function to calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e575937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    # Convert both sentences to lowercase\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    \n",
    "    # Combine both sentences to create a list of all unique words\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    \n",
    "    # Initialize vectors to represent word frequency in each sentence\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "        \n",
    "    # Calculate cosine similarity between the two vectors\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea433a0",
   "metadata": {},
   "source": [
    "## Create the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f622da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a square matrix to store similarity scores between sentences\n",
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "# Loop through each pair of sentences\n",
    "for idx1 in range(len(sentences)):\n",
    "    for idx2 in range(len(sentences)):\n",
    "        if idx1 == idx2: #ignore if both are same sentences\n",
    "            continue\n",
    "        # Calculate the similarity score between the current pair of sentences\n",
    "        similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "\n",
    "# Print the similarity matrix\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f7127",
   "metadata": {},
   "source": [
    "## Get the pagerank scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e4fc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 0.0252111715444404, 1: 0.0252111715444404, 2: 0.0252111715444404, 3: 0.0252111715444404, 4: 0.0252111715444404, 5: 0.0252111715444404, 6: 0.16806369248056863, 7: 0.0252111715444404, 8: 0.16806369248056863, 9: 0.0252111715444404, 10: 0.0252111715444404, 11: 0.16806369248056863, 12: 0.0252111715444404, 13: 0.16806369248056863, 14: 0.0252111715444404, 15: 0.0252111715444404, 16: 0.0252111715444404}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "\n",
    "# Convert the similarity matrix to a graph\n",
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "# Calculate the PageRank scores for each node (sentence) in the graph\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "# Print the PageRank scores for each sentence\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c6986",
   "metadata": {},
   "source": [
    "## Sort sentences by pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "228c074c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(0.16806369248056863, ['～']), (0.16806369248056863, ['～']), (0.16806369248056863, ['年)']), (0.16806369248056863, ['年)']), (0.0252111715444404, ['\\ufeffこの言語の先史や、いつ日本に初めて登場したかについてはほとんど知られていません。西暦']), (0.0252111715444404, ['年に日本の自主的鎖国が終了した後、ヨーロッパ言語からの外来語の流れが大幅に増加し、英語のルーツに由来する単語が急増しました。\\n']), (0.0252111715444404, ['世紀まで出現しませんでした。平安時代']), (0.0252111715444404, ['世紀の中国の文書にはいくつかの日本語の単語が記録されていましたが、実質的な古日本語の文書は']), (0.0252111715444404, ['では、文法が大幅に変更され、ヨーロッパからの外来語が初めて登場しました。標準語の基礎は近世日本の時代（17世紀前半～19世紀半ば）に関西地方から江戸地方（現在の東京）に移りました。']), (0.0252111715444404, ['から、中国と日本の語彙が大量に流入し、中期初期日本語の音韻論に影響を与えました。中期後期日本語']), (0.0252111715444404, ['8']), (0.0252111715444404, ['3']), (0.0252111715444404, ['1853']), (0.0252111715444404, ['1600']), (0.0252111715444404, ['1185']), (0.0252111715444404, ['(794']), (0.0252111715444404, ['(1185'])]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in\n",
    "enumerate(sentences)), reverse=True)\n",
    "\n",
    "# Print the indexes and sentences of the top-ranked sentences in order\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",\n",
    "ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b015ff",
   "metadata": {},
   "source": [
    "## Pick the top \"n\" sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ddec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary? 4\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "\n",
    "# Prompt the user to input the number of sentences they want in the summary\n",
    "n = int(input(\"How many sentences do you want in the summary? \")) # n=4\n",
    "\n",
    "# Initialize an empty list to store the summarized sentences\n",
    "summarize_text = []\n",
    "\n",
    "# Iterate over the top n ranked sentences\n",
    "for i in range(n):\n",
    "    \n",
    "    # Append the sentence to the summarized text list, joining the words into a single string\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ace09",
   "metadata": {},
   "source": [
    "## Finish off by printing summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb30796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " ～. ～. 年). 年)\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - # Output the summarized text by joining the sentences into a single string with a period and space between each sentence\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d45cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a81f00c4",
   "metadata": {},
   "source": [
    "## Korean Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d48c1f",
   "metadata": {},
   "source": [
    "## Import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bda9851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the stopwords corpus from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Importing the cosine_distance function from NLTK cluster utility\n",
    "from nltk.cluster.util import cosine_distance\n",
    "\n",
    "# Importing numpy library and aliasing it as 'np'\n",
    "import numpy as np\n",
    "\n",
    "# Importing the networkx library and aliasing it as 'nx'\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b30ae",
   "metadata": {},
   "source": [
    "## Open file and split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14d61d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿この言語の先史や、いつ日本に初めて登場したかについてはほとんど知られていません。西暦 3 世紀の中国の文書にはいくつかの日本語の単語が記録されていましたが、実質的な古日本語の文書は 8 世紀まで出現しませんでした。平安時代 (794 ～ 1185 年) から、中国と日本の語彙が大量に流入し、中期初期日本語の音韻論に影響を与えました。中期後期日本語 (1185 ～ 1600 年) では、文法が大幅に変更され、ヨーロッパからの外来語が初めて登場しました。標準語の基礎は近世日本の時代（17世紀前半～19世紀半ば）に関西地方から江戸地方（現在の東京）に移りました。 1853 年に日本の自主的鎖国が終了した後、ヨーロッパ言語からの外来語の流れが大幅に増加し、英語のルーツに由来する単語が急増しました。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Opening the text file in read mode\n",
    "file = open(\"C:\\\\Users\\\\himav\\\\Downloads\\\\Japanese.txt\", encoding=\"utf-8\")\n",
    "\n",
    "#This file contains one paragraph of multiple sentences\n",
    "filedata = file.readlines()\n",
    "\n",
    "# Extracting the first paragraph from the file data and splitting it into sentences\n",
    "article = filedata[0].split(\". \")\n",
    "\n",
    "# Initializing an empty list to store the tokenized sentences\n",
    "sentences = []\n",
    "\n",
    "# Looping through each sentence in the article\n",
    "for sentence in article:\n",
    "    # Printing the sentence\n",
    "    print(sentence)\n",
    "    # Tokenizing the sentence by replacing non-alphabetic characters with spaces and then splitting on spaces\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663b8b5",
   "metadata": {},
   "source": [
    "## Our data: a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac7c4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['\\ufeffこの言語の先史や、いつ日本に初めて登場したかについてはほとんど知られていません。西暦', '3', '世紀の中国の文書にはいくつかの日本語の単語が記録されていましたが、実質的な古日本語の文書は', '8', '世紀まで出現しませんでした。平安時代', '(794', '～', '1185', '年)', 'から、中国と日本の語彙が大量に流入し、中期初期日本語の音韻論に影響を与えました。中期後期日本語', '(1185', '～', '1600', '年)', 'では、文法が大幅に変更され、ヨーロッパからの外来語が初めて登場しました。標準語の基礎は近世日本の時代（17世紀前半～19世紀半ば）に関西地方から江戸地方（現在の東京）に移りました。', '1853', '年に日本の自主的鎖国が終了した後、ヨーロッパ言語からの外来語の流れが大幅に増加し、英語のルーツに由来する単語が急増しました。\\n']]\n"
     ]
    }
   ],
   "source": [
    "# Printing the list of tokenized sentences\n",
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924463b4",
   "metadata": {},
   "source": [
    "## Function to calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c92df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    # Convert both sentences to lowercase\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    \n",
    "    # Combine both sentences to create a list of all unique words\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    \n",
    "    # Initialize vectors to represent word frequency in each sentence\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "        \n",
    "    # Calculate cosine similarity between the two vectors\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d14e97",
   "metadata": {},
   "source": [
    "## Create the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a702a2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a square matrix to store similarity scores between sentences\n",
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "# Loop through each pair of sentences\n",
    "for idx1 in range(len(sentences)):\n",
    "    for idx2 in range(len(sentences)):\n",
    "        if idx1 == idx2: #ignore if both are same sentences\n",
    "            continue\n",
    "        # Calculate the similarity score between the current pair of sentences\n",
    "        similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "\n",
    "# Print the similarity matrix\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fdc58",
   "metadata": {},
   "source": [
    "## Get the pagerank scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf74d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "\n",
    "# Convert the similarity matrix to a graph\n",
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "# Calculate the PageRank scores for each node (sentence) in the graph\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "# Print the PageRank scores for each sentence\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd97ea3",
   "metadata": {},
   "source": [
    "## Sort sentences by pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e178c0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(1.0, ['\\ufeffこの言語の先史や、いつ日本に初めて登場したかについてはほとんど知られていません。西暦', '3', '世紀の中国の文書にはいくつかの日本語の単語が記録されていましたが、実質的な古日本語の文書は', '8', '世紀まで出現しませんでした。平安時代', '(794', '～', '1185', '年)', 'から、中国と日本の語彙が大量に流入し、中期初期日本語の音韻論に影響を与えました。中期後期日本語', '(1185', '～', '1600', '年)', 'では、文法が大幅に変更され、ヨーロッパからの外来語が初めて登場しました。標準語の基礎は近世日本の時代（17世紀前半～19世紀半ば）に関西地方から江戸地方（現在の東京）に移りました。', '1853', '年に日本の自主的鎖国が終了した後、ヨーロッパ言語からの外来語の流れが大幅に増加し、英語のルーツに由来する単語が急増しました。\\n'])]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in\n",
    "enumerate(sentences)), reverse=True)\n",
    "\n",
    "# Print the indexes and sentences of the top-ranked sentences in order\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",\n",
    "ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba295c80",
   "metadata": {},
   "source": [
    "## Pick the top \"n\" sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bdaf1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary? 1\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "\n",
    "# Prompt the user to input the number of sentences they want in the summary\n",
    "n = int(input(\"How many sentences do you want in the summary? \")) # n=1\n",
    "\n",
    "# Initialize an empty list to store the summarized sentences\n",
    "summarize_text = []\n",
    "\n",
    "# Iterate over the top n ranked sentences\n",
    "for i in range(n):\n",
    "    \n",
    "    # Append the sentence to the summarized text list, joining the words into a single string\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39c698",
   "metadata": {},
   "source": [
    "## Finish off by printing summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd0ae013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " ﻿この言語の先史や、いつ日本に初めて登場したかについてはほとんど知られていません。西暦 3 世紀の中国の文書にはいくつかの日本語の単語が記録されていましたが、実質的な古日本語の文書は 8 世紀まで出現しませんでした。平安時代 (794 ～ 1185 年) から、中国と日本の語彙が大量に流入し、中期初期日本語の音韻論に影響を与えました。中期後期日本語 (1185 ～ 1600 年) では、文法が大幅に変更され、ヨーロッパからの外来語が初めて登場しました。標準語の基礎は近世日本の時代（17世紀前半～19世紀半ば）に関西地方から江戸地方（現在の東京）に移りました。 1853 年に日本の自主的鎖国が終了した後、ヨーロッパ言語からの外来語の流れが大幅に増加し、英語のルーツに由来する単語が急増しました。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - # Output the summarized text by joining the sentences into a single string with a period and space between each sentence\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a3dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15eeacee",
   "metadata": {},
   "source": [
    "## Chinese Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6854f",
   "metadata": {},
   "source": [
    "## Import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a0b708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the stopwords corpus from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Importing the cosine_distance function from NLTK cluster utility\n",
    "from nltk.cluster.util import cosine_distance\n",
    "\n",
    "# Importing numpy library and aliasing it as 'np'\n",
    "import numpy as np\n",
    "\n",
    "# Importing the networkx library and aliasing it as 'nx'\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0268f2",
   "metadata": {},
   "source": [
    "## Open file and split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9764b16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿漢語是世界上最古老的文字，至少有六千年的歷史。在龜殼中發現的漢字銘文可以追溯到商代1（公元前 1766-1123 年），證明書面語言已經存在了 3000 多年。中文書面語言使用單一獨特的符號或字元來表示詞彙中的每個單字。絕大多數字符都是有意義的口頭聲音的書面版本。一本大字典通常包含 40,000 個字元。儘管隨著時間的推移，由於革命和政治變化，書面系統發生了變化，但語言的原理以及符號和字符基本上保持不變。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Opening the text file in read mode\n",
    "file = open(\"C:\\\\Users\\\\himav\\\\Downloads\\\\Chinese.txt\", encoding=\"utf-8\")\n",
    "\n",
    "#This file contains one paragraph of multiple sentences\n",
    "filedata = file.readlines()\n",
    "\n",
    "# Extracting the first paragraph from the file data and splitting it into sentences\n",
    "article = filedata[0].split(\". \")\n",
    "\n",
    "# Initializing an empty list to store the tokenized sentences\n",
    "sentences = []\n",
    "\n",
    "# Looping through each sentence in the article\n",
    "for sentence in article:\n",
    "    # Printing the sentence\n",
    "    print(sentence)\n",
    "    # Tokenizing the sentence by replacing non-alphabetic characters with spaces and then splitting on spaces\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43cbc99",
   "metadata": {},
   "source": [
    "## Our data: a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51430f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['\\ufeff漢語是世界上最古老的文字，至少有六千年的歷史。在龜殼中發現的漢字銘文可以追溯到商代1（公元前', '1766-1123', '年），證明書面語言已經存在了', '3000', '多年。中文書面語言使用單一獨特的符號或字元來表示詞彙中的每個單字。絕大多數字符都是有意義的口頭聲音的書面版本。一本大字典通常包含', '40,000', '個字元。儘管隨著時間的推移，由於革命和政治變化，書面系統發生了變化，但語言的原理以及符號和字符基本上保持不變。\\n']]\n"
     ]
    }
   ],
   "source": [
    "# Printing the list of tokenized sentences\n",
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9080be2",
   "metadata": {},
   "source": [
    "## Function to calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cc45516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    # Convert both sentences to lowercase\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    \n",
    "    # Combine both sentences to create a list of all unique words\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    \n",
    "    # Initialize vectors to represent word frequency in each sentence\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "        \n",
    "    # Calculate cosine similarity between the two vectors\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8914cbd",
   "metadata": {},
   "source": [
    "## Create the similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e38b18b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a square matrix to store similarity scores between sentences\n",
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "# Loop through each pair of sentences\n",
    "for idx1 in range(len(sentences)):\n",
    "    for idx2 in range(len(sentences)):\n",
    "        if idx1 == idx2: #ignore if both are same sentences\n",
    "            continue\n",
    "        # Calculate the similarity score between the current pair of sentences\n",
    "        similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "\n",
    "# Print the similarity matrix\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac486745",
   "metadata": {},
   "source": [
    "## Get the pagerank scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be57c711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "\n",
    "# Convert the similarity matrix to a graph\n",
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "# Calculate the PageRank scores for each node (sentence) in the graph\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "# Print the PageRank scores for each sentence\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4626b8",
   "metadata": {},
   "source": [
    "## Sort sentences by pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4794c17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(1.0, ['\\ufeff漢語是世界上最古老的文字，至少有六千年的歷史。在龜殼中發現的漢字銘文可以追溯到商代1（公元前', '1766-1123', '年），證明書面語言已經存在了', '3000', '多年。中文書面語言使用單一獨特的符號或字元來表示詞彙中的每個單字。絕大多數字符都是有意義的口頭聲音的書面版本。一本大字典通常包含', '40,000', '個字元。儘管隨著時間的推移，由於革命和政治變化，書面系統發生了變化，但語言的原理以及符號和字符基本上保持不變。\\n'])]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in\n",
    "enumerate(sentences)), reverse=True)\n",
    "\n",
    "# Print the indexes and sentences of the top-ranked sentences in order\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",\n",
    "ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a676e37",
   "metadata": {},
   "source": [
    "## Pick the top \"n\" sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "982b7287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary? 1\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "\n",
    "# Prompt the user to input the number of sentences they want in the summary\n",
    "n = int(input(\"How many sentences do you want in the summary? \")) # n=1\n",
    "\n",
    "# Initialize an empty list to store the summarized sentences\n",
    "summarize_text = []\n",
    "\n",
    "# Iterate over the top n ranked sentences\n",
    "for i in range(n):\n",
    "    \n",
    "    # Append the sentence to the summarized text list, joining the words into a single string\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a18a5",
   "metadata": {},
   "source": [
    "## Finish off by printing summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e71fea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " ﻿漢語是世界上最古老的文字，至少有六千年的歷史。在龜殼中發現的漢字銘文可以追溯到商代1（公元前 1766-1123 年），證明書面語言已經存在了 3000 多年。中文書面語言使用單一獨特的符號或字元來表示詞彙中的每個單字。絕大多數字符都是有意義的口頭聲音的書面版本。一本大字典通常包含 40,000 個字元。儘管隨著時間的推移，由於革命和政治變化，書面系統發生了變化，但語言的原理以及符號和字符基本上保持不變。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Output the summarized text by joining the sentences into a single string with a period and space between each sentence\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af1c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
